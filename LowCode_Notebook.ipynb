{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EaJ8AGwpM-2"
      },
      "source": [
        "# **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3-QehJxbp0t"
      },
      "source": [
        "## Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Renewable energy sources play an increasingly important role in the global energy mix, as the effort to reduce the environmental impact of energy production increases.\n",
        "\n",
        "Out of all the renewable energy alternatives, wind energy is one of the most developed technologies worldwide. The U.S Department of Energy has put together a guide to achieving operational efficiency using predictive maintenance practices.\n",
        "\n",
        "Predictive maintenance uses sensor information and analysis methods to measure and predict degradation and future component capability. The idea behind predictive maintenance is that failure patterns are predictable and if component failure can be predicted accurately and the component is replaced before it fails, the costs of operation and maintenance will be much lower.\n",
        "\n",
        "The sensors fitted across different machines involved in the process of energy generation collect data related to various environmental factors (temperature, humidity, wind speed, etc.) and additional features related to various parts of the wind turbine (gearbox, tower, blades, break, etc.)."
      ],
      "metadata": {
        "id": "7DoY1AHZfMF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective"
      ],
      "metadata": {
        "id": "4p9zp6bHfPOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“ReneWind” is a company working on improving the machinery/processes involved in the production of wind energy using machine learning and has collected data of generator failure of wind turbines using sensors. They have shared a ciphered version of the data, as the data collected through sensors is confidential (the type of data collected varies with companies). Data has 40 predictors, 20000 observations in the training set and 5000 in the test set.\n",
        "\n",
        "The objective is to build various classification models, tune them, and find the best one that will help identify failures so that the generators could be repaired before failing/breaking to reduce the overall maintenance cost.\n",
        "The nature of predictions made by the classification model will translate as follows:\n",
        "\n",
        "- True positives (TP) are failures correctly predicted by the model. These will result in repairing costs.\n",
        "- False negatives (FN) are real failures where there is no detection by the model. These will result in replacement costs.\n",
        "- False positives (FP) are detections where there is no failure. These will result in inspection costs.\n",
        "\n",
        "It is given that the cost of repairing a generator is much less than the cost of replacing it, and the cost of inspection is less than the cost of repair.\n",
        "\n",
        "“1” in the target variables should be considered as “failure” and “0” represents “No failure”."
      ],
      "metadata": {
        "id": "eu7YIw90fRdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Description"
      ],
      "metadata": {
        "id": "XPeJAS5bfUOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data provided is a transformed version of the original data which was collected using sensors.\n",
        "\n",
        "- Train.csv - To be used for training and tuning of models.\n",
        "- Test.csv - To be used only for testing the performance of the final best model.\n",
        "\n",
        "Both the datasets consist of 40 predictor variables and 1 target variable."
      ],
      "metadata": {
        "id": "P4ns9KiyfVsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Please read the instructions carefully before starting the project.**\n",
        "This is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "* Blanks '_______' are provided in the notebook that\n",
        "needs to be filled with an appropriate code to get the correct result. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space.\n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\". Running incomplete code may throw error.\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same."
      ],
      "metadata": {
        "id": "wesRJFbQrTs7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_-uuGqH-qTt"
      },
      "source": [
        "# **Installing and Importing the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob7CHevooE2s"
      },
      "outputs": [],
      "source": [
        "# Installing the libraries with the specified version\n",
        "!pip install --no-deps tensorflow==2.18.0 scikit-learn==1.3.2 matplotlib===3.8.3 seaborn==0.13.2 numpy==1.26.4 pandas==2.2.2 -q --user --no-warn-script-location"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."
      ],
      "metadata": {
        "id": "QKE5LWqhvChF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbiVVX6hbp0v"
      },
      "outputs": [],
      "source": [
        "# Library for data manipulation and analysis.\n",
        "import pandas as pd\n",
        "# Fundamental package for scientific computing.\n",
        "import numpy as np\n",
        "#splitting datasets into training and testing sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Imports tools for data preprocessing including label encoding, one-hot encoding, and standard scaling\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder,StandardScaler\n",
        "#Imports a class for imputing missing values in datasets.\n",
        "from sklearn.impute import SimpleImputer\n",
        "#Imports the Matplotlib library for creating visualizations.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "# Imports the Seaborn library for statistical data visualization.\n",
        "import seaborn as sns\n",
        "# Time related functions.\n",
        "import time\n",
        "#Imports functions for evaluating the performance of machine learning models\n",
        "from sklearn.metrics import confusion_matrix, f1_score,accuracy_score, recall_score, precision_score, classification_report\n",
        "#Imports metrics from\n",
        "from sklearn import metrics\n",
        "\n",
        "#Imports the tensorflow,keras and layers.\n",
        "import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,BatchNormalization\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "# to suppress unnecessary warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxhpZv9y-qTw"
      },
      "source": [
        "# **Loading the Data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment and run the following lines for Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "D4QgYmaOkMEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39UgpBY3bp0y"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"_______\")    # Complete the code to import the training data\n",
        "df_test = pd.read_csv(\"_______\")    # Complete the code to import the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cVx5kZHbp02"
      },
      "source": [
        "# **Data Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-5jUOgu-qTz"
      },
      "source": [
        "The initial steps to get an overview of any dataset is to:\n",
        "- observe the first few rows of the dataset, to check whether the dataset has been loaded properly or not\n",
        "- get information about the number of rows and columns in the dataset\n",
        "- find out the data types of the columns to ensure that data is stored in the preferred format and the value of each property is as expected.\n",
        "- check the statistical summary of the dataset to get an overview of the numerical columns of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQi5ygTC-qT1"
      },
      "source": [
        "## Checking the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y8epHpjbp0z"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the training data\n",
        "df.______ # Complete the code to print the shape of the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA15qTjzbp01"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the test data\n",
        "df_test._______ # Complete the code to print the shape of the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTsYJZBubp02"
      },
      "outputs": [],
      "source": [
        "# let's create a copy of the training data\n",
        "data = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ImWbLXbp03"
      },
      "outputs": [],
      "source": [
        "# let's create a copy of the testing  data\n",
        "data_test = df_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlzqMR1K-qTz"
      },
      "source": [
        "## Displaying the first few rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9RnN7Twbp03"
      },
      "outputs": [],
      "source": [
        "# let's view the first 5 rows of the data\n",
        "data.________ # Complete the code to view the first five rows of the train data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#viewing first 5 rows of the test data\n",
        "data_test.______ # Complete the code to view the first five rows of the test data"
      ],
      "metadata": {
        "id": "47OhpabimjMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TcqcxbK-qT3"
      },
      "source": [
        "## Checking the data types of the columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXBYJoKkbp04"
      },
      "outputs": [],
      "source": [
        "# let's check the data types of the columns in the dataset\n",
        "data.________ # Complete the code to view the data types of the columns in the train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvN3_-Vdbp04"
      },
      "source": [
        "- Converting Target column to float"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Target'] = data['Target'].astype(float)"
      ],
      "metadata": {
        "id": "6LwVuEgjmHxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now checking for test data"
      ],
      "metadata": {
        "id": "4MmheoOtnYZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_test.________ # Complete the code to view the data types of the columns in the test data"
      ],
      "metadata": {
        "id": "Utfm_wZsnR1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Target to float"
      ],
      "metadata": {
        "id": "KyX9vFVFnk5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_test['Target'] = data_test['Target'].astype(float)"
      ],
      "metadata": {
        "id": "SfTJuArBnbIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNr4bWoM-qT5"
      },
      "source": [
        "## Checking for duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0EmBHNmbp04"
      },
      "outputs": [],
      "source": [
        "# let's check for duplicate values in the data\n",
        "data.________ # Complete the code to check for duplicate values in the train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch_TjRfF-qT5"
      },
      "source": [
        "## Checking for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlwFZm-Jbp05"
      },
      "outputs": [],
      "source": [
        "# let's check for missing values in the data\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33bar6robp06"
      },
      "outputs": [],
      "source": [
        "# let's check for missing values in the test data\n",
        "data_test.isnull().sum() #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUCorhch-qT4"
      },
      "source": [
        "## Statistical summary of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6lzvHKCbp06"
      },
      "outputs": [],
      "source": [
        "# let's view the statistical summary of the numerical columns in the data\n",
        "data.________ # Complete the code to view the statistical summary of the train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPuzWO7hmV8"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv7Bs8aUbp07"
      },
      "source": [
        "## Univariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIP4bI3Zbp07"
      },
      "outputs": [],
      "source": [
        "# function to plot a boxplot and a histogram along the same scale.\n",
        "\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data= ______, x= _______, ax= _______, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=______, x=_______, kde=kde, ax=_______, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=________, x=________, kde=kde, ax=______\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables V1 to V29"
      ],
      "metadata": {
        "id": "_5N52iqsJBeb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0ip-InCbp07",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for feature in df.columns:\n",
        "    histogram_boxplot(df, feature, figsize=(12, 7), kde=False, bins=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CjaGOdvbp08"
      },
      "source": [
        "### Checking the distrubution of Target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jchAb6Ikbp08"
      },
      "outputs": [],
      "source": [
        "# For train data\n",
        "df[\"Target\"].value_counts(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rupsM4sbp08"
      },
      "outputs": [],
      "source": [
        "# For test data\n",
        "df_test[\"Target\"]._____ # Complete the code to display the proportion of the target variable in the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bivariate Analysis"
      ],
      "metadata": {
        "id": "lRrYccqLJKXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation Check"
      ],
      "metadata": {
        "id": "Lk0t8FYwJMeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_list = df.select_dtypes(include=np.number).columns.tolist()\n",
        "cols_list.remove(\"Target\")\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(\n",
        "    df[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kL32Bf4rJO1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp8vC9MZbp09"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMTLLop3yDwk"
      },
      "source": [
        "## Data Preparation for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-xgnTbGb4CY"
      },
      "outputs": [],
      "source": [
        "# Dividing train data into X and y\n",
        "X = data.drop(columns = [\"_______\"] , axis=1) # Complete the code to remove the column named 'Target'\n",
        "y = data[\"________\"] # Complete the code to select the column named 'Target'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBXiW4s4kd63"
      },
      "source": [
        "**Since we already have a separate test set, we don't need to divide data into train, valiation and test**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-8vqBMXbp09"
      },
      "outputs": [],
      "source": [
        "# Splitting data into training and validation set:\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=_____, random_state=1, stratify=y # Complete the code to define the test size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIsrQQVqRbnY"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the X_train data\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3_X_6ZicNfN"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the X_val data\n",
        "X_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IktO9G2Lbp09"
      },
      "outputs": [],
      "source": [
        "# Dividing test data into X_test and y_test\n",
        "X_test = data_test.drop(columns = ['_______'] , axis= 1) # Complete the code to remove the target column\n",
        "y_test = data_test[\"______\"] # Complete the code to select the target column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmuB5K_9b8tP"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the X_test data\n",
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRIcs_56ECHk"
      },
      "source": [
        "## Missing Value Imputation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J99-7Kubp09"
      },
      "source": [
        "- There were few missing values in V1 and V2, we will impute them using the median.\n",
        "- And to avoid data leakage we will impute missing values after splitting train data into train and validation sets.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28L1vgAwbp09"
      },
      "outputs": [],
      "source": [
        "imputer = SimpleImputer(strategy=\"median\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNfbw4rJbp09"
      },
      "outputs": [],
      "source": [
        "# Fit and transform the train data\n",
        "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "\n",
        "# Transform the validation data\n",
        "X_val = pd.DataFrame(imputer.______(X_val), columns=X_train.columns)    # Complete the code to impute missing values in the validation set while accounting for data leakage\n",
        "\n",
        "# Transform the test data\n",
        "X_test = pd.DataFrame(imputer.______(X_test), columns=X_train.columns)    # Complete the code to impute missing values in the test set while accounting for data leakage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jdLvwTAbp09"
      },
      "outputs": [],
      "source": [
        "# Checking that no column has missing values in train or test sets\n",
        "print(X_train.isna().sum())\n",
        "print(\"-\" * 30)\n",
        "print(X_val.isna().sum())\n",
        "print(\"-\" * 30)\n",
        "print(X_test.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.to_numpy()\n",
        "y_val = y_val.to_numpy()\n",
        "y_test = y_test.to_numpy()"
      ],
      "metadata": {
        "id": "vdAf7_aNRpyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzOa9FGA6WtG"
      },
      "source": [
        "# **Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czwxDIZNccQL"
      },
      "source": [
        "## Model Evaluation Criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ORUgmUjDZC"
      },
      "source": [
        "- Write down the metric of choice with rationale here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWoHuUpjbp0_"
      },
      "source": [
        "**We are now done with pre-processing and evaluation criterion, so let's start building the model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "hoFZRy7tSXnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(history, name):\n",
        "    \"\"\"\n",
        "    Function to plot loss/accuracy\n",
        "\n",
        "    history: an object which stores the metrics and losses.\n",
        "    name: can be one of Loss or Accuracy\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots() #Creating a subplot with figure and axes.\n",
        "    plt.plot(history.history[name]) #Plotting the train accuracy or train loss\n",
        "    plt.plot(history.history['val_'+name]) #Plotting the validation accuracy or validation loss\n",
        "\n",
        "    plt.title('Model ' + name.capitalize()) #Defining the title of the plot.\n",
        "    plt.ylabel(name.capitalize()) #Capitalizing the first letter.\n",
        "    plt.xlabel('Epoch') #Defining the label for the x-axis.\n",
        "    fig.legend(['Train', 'Validation'], loc=\"outside right upper\") #Defining the legend, loc controls the position of the legend."
      ],
      "metadata": {
        "id": "fpMmFI9USZYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using statsmodels\n",
        "def model_performance_classification(\n",
        "    model, predictors, target, threshold=0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    threshold: threshold for classifying the observation as class 1\n",
        "    \"\"\"\n",
        "\n",
        "    # checking which probabilities are greater than threshold\n",
        "    pred = model.predict(predictors) > threshold\n",
        "    # pred_temp = model.predict(predictors) > threshold\n",
        "    # # rounding off the above values to get classes\n",
        "    # pred = np.round(pred_temp)\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred, average='macro')  # to compute Recall\n",
        "    precision = precision_score(target, pred, average='macro')  # to compute Precision\n",
        "    f1 = f1_score(target, pred, average='macro')  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1 Score\": f1,}, index = [0]\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ],
      "metadata": {
        "id": "H_VAOcNcScFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fI98GOV0pTY"
      },
      "source": [
        "## Initial Model Building (Model 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's start with a neural network consisting of\n",
        "  - just one hidden layer of 7 neurons respectively\n",
        "  - activation function of ReLU.\n",
        "  - SGD as the optimizer"
      ],
      "metadata": {
        "id": "0usUom8nZOYZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OTOG3o1bp0_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# defining the batch size and # epochs upfront as we'll be using the same values for all models\n",
        "epochs = ____    # Complete the code to enter the number of epochs to be used in all models\n",
        "batch_size = _____    # Complete the code to enter the batch size to be used in all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm61fR_mbp0_",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the neural network\n",
        "model_0 = Sequential()\n",
        "model_0.add(Dense( _____ ,activation=\"_______\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and the activation function\n",
        "model_0.add(Dense( _____ ,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ],
      "metadata": {
        "id": "ao_7u1sOZlZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2dEk01hbp1A"
      },
      "outputs": [],
      "source": [
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsOer36qbp1A"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "# model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwUcBORjbp1A",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_0.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ],
      "metadata": {
        "id": "zwBB3mRhcCN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(history,'loss')"
      ],
      "metadata": {
        "id": "RfLx8ubqTe_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_train_perf = model_performance_classification(model_0, X_train, y_train)\n",
        "model_0_train_perf"
      ],
      "metadata": {
        "id": "Pr6dBVPNUe3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_val_perf = model_performance_classification(model_0,X_val,y_val)\n",
        "model_0_val_perf"
      ],
      "metadata": {
        "id": "1pdgAup2UfAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the classification reports."
      ],
      "metadata": {
        "id": "7757b8PflQ9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_0 = model_0.predict(X_train)\n",
        "y_val_pred_0 = model_0.predict(X_val)"
      ],
      "metadata": {
        "id": "m0UQ_ZW-KCaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Train data Model_0\",end=\"\\n\\n\")\n",
        "cr_train_model_0 = classification_report(y_train,y_train_pred_0>0.5)\n",
        "print(cr_train_model_0)"
      ],
      "metadata": {
        "id": "zC6fR5ohKFzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Validation data Model_0\",end=\"\\n\\n\")\n",
        "cr_val_model_0 = classification_report(y_val,y_val_pred_0>0.5)\n",
        "print(cr_val_model_0)"
      ],
      "metadata": {
        "id": "foiSZUcHKFp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Performance Improvement**"
      ],
      "metadata": {
        "id": "2tlVEbIWKb6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1"
      ],
      "metadata": {
        "id": "grQ6pRK1WZtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's try adding another layer to see if we can improve our model's performance."
      ],
      "metadata": {
        "id": "zBr3Eo1iWhLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "E95a3_v6UfE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the neural network\n",
        "model_1 = Sequential()\n",
        "model_1.add(Dense( ________ ,activation=\"________\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_1.add(Dense( ________,activation=\"________\")) # Complete the code to define the number of neurons and activation function\n",
        "model_1.add(Dense(_______,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ],
      "metadata": {
        "id": "uzB8avdtUfH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "id": "cnuMH8oQUfKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "# model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ],
      "metadata": {
        "id": "xfDQJHYnUfM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "history = model_1.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ],
      "metadata": {
        "id": "HsPSnSmNUfO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ],
      "metadata": {
        "id": "GX9bBylsUfRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(history,'loss')"
      ],
      "metadata": {
        "id": "pKXH3xAYUfTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_train_perf = model_performance_classification(model_1,X_train,y_train)\n",
        "model_1_train_perf"
      ],
      "metadata": {
        "id": "Ve7QYfyTUfXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_val_perf = model_performance_classification(model_1,X_val,y_val)\n",
        "model_1_val_perf"
      ],
      "metadata": {
        "id": "09KWUhmONOEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_1 = model_1.predict(X_train)\n",
        "y_val_pred_1 = model_1.predict(X_val)"
      ],
      "metadata": {
        "id": "DS8SFCwJUoe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Train data Model_1\", end=\"\\n\\n\")\n",
        "cr_train_model_1 = classification_report(y_train,y_train_pred_1 > 0.5)\n",
        "print(cr_train_model_1)"
      ],
      "metadata": {
        "id": "8ZngTFbEUskK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Validation data Model_1\", end=\"\\n\\n\")\n",
        "cr_val_model_1 = classification_report(y_val,y_val_pred_1 > 0.5)\n",
        "print(cr_val_model_1)"
      ],
      "metadata": {
        "id": "Ju0Rzn1kUtRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2"
      ],
      "metadata": {
        "id": "iQhdyBg0OJWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To introduce Regularization in our model, let's set the dropout to 50% after adding the first hidden layer. This step will randomly drop 50% of the neurons before proceeding to the next layer, reducing overfitting."
      ],
      "metadata": {
        "id": "jUwTGnuGfgRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "-7mdqo7xOO_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the neural network\n",
        "from tensorflow.keras.layers import Dropout\n",
        "model_2 = Sequential()\n",
        "model_2.add(Dense(________,activation=\"_________\",input_dim=X_train.shape[1]))  # Complete the code to define the number of neurons and activation function\n",
        "model_2.add(Dropout(____)) # Complete the code to define the dropout rate\n",
        "model_2.add(Dense(_____,activation = \"______\")) # Complete the code to define the number of neurons and activation function\n",
        "model_2.add(Dense(_____,activation = \"______\")) # Complete the code to define the number of neurons and activation function\n",
        "model_2.add(Dense(_____,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ],
      "metadata": {
        "id": "TXbGBf1QOO9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "id": "nPu7lC9pOO23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "# model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ],
      "metadata": {
        "id": "U7-Y_BmYOOxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "history = model_2.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ],
      "metadata": {
        "id": "_QPtdXotOOu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ],
      "metadata": {
        "id": "a_VLN5_FOOss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(history,'loss')"
      ],
      "metadata": {
        "id": "1yat6HgCOOmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the model performance of model_2 on training and validation data respectively."
      ],
      "metadata": {
        "id": "fdmnaDUxbVvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_train_perf = model_performance_classification(model_2,X_train,y_train)\n",
        "model_2_train_perf"
      ],
      "metadata": {
        "id": "lhHFHpRJOOf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_val_perf = model_performance_classification(model_2,X_val,y_val)\n",
        "model_2_val_perf"
      ],
      "metadata": {
        "id": "pnMTEzWtOOW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_2 = model_2.predict(X_train)\n",
        "y_val_pred_2 = model_2.predict(X_val)"
      ],
      "metadata": {
        "id": "DI5dqTACWC5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the classification report of model_2 on training and validation data respectively."
      ],
      "metadata": {
        "id": "XZOkixQ9bfnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Train data Model_2\", end=\"\\n\\n\")\n",
        "cr_train_model_2 = classification_report(y_train,y_train_pred_2 > 0.5)\n",
        "print(cr_train_model_2)"
      ],
      "metadata": {
        "id": "TPtFhE5GWAFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Validation data Model_2\", end=\"\\n\\n\")\n",
        "cr_val_model_2 = classification_report(y_val , y_val_pred_2 > 0.5)\n",
        "print(cr_val_model_2)"
      ],
      "metadata": {
        "id": "cE-wWumtWEpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3"
      ],
      "metadata": {
        "id": "83ryNh6qVVPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have are dealing with an imbalance in class distribution, we should also be using class weights to allow the model to give proportionally more importance to the minority class."
      ],
      "metadata": {
        "id": "swQ14MhSRwCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class weights for imbalanced dataset\n",
        "cw = (y_train.shape[0]) / np.bincount(y_train.astype(int)) # Convert y_train to integers\n",
        "\n",
        "# Create a dictionary mapping class indices to their respective class weights\n",
        "cw_dict = {}\n",
        "for i in range(cw.shape[0]):\n",
        "    cw_dict[i] = cw[i]\n",
        "\n",
        "cw_dict"
      ],
      "metadata": {
        "id": "bXFEE44vOkYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "hof3fAu9OkWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = Sequential()\n",
        "model_3.add(Dense(_____,activation=\"_____\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_3.add(Dropout(_____)) # Complete the code to define the dropout rate\n",
        "model_3.add(Dense(_____,activation=\"_____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_3.add(Dense(_____, activation = \"_____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_3.add(Dense(_____,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ],
      "metadata": {
        "id": "PWebuv4mOkTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.summary()"
      ],
      "metadata": {
        "id": "CN5XaqlQOkL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "# model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ],
      "metadata": {
        "id": "Vbe-Uvf_OkBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "history = model_3.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs,class_weight=cw_dict)\n",
        "end=time.time()"
      ],
      "metadata": {
        "id": "aGEYYC5pW5iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ],
      "metadata": {
        "id": "-H-rbOvIW5gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(history,'loss')"
      ],
      "metadata": {
        "id": "aWkRSkTGW5dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the model performance of model_3 on training and validation data respectively."
      ],
      "metadata": {
        "id": "dY6ZCgMGcUOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_train_perf = model_performance_classification(model_3,X_train,y_train)\n",
        "model_3_train_perf"
      ],
      "metadata": {
        "id": "_fryTkbgW5XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_val_perf = model_performance_classification(model_3,X_val,y_val)\n",
        "model_3_val_perf"
      ],
      "metadata": {
        "id": "GxmCeXa1W5Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_3 = model_3.predict(X_train)\n",
        "y_val_pred_3 = model_3.predict(X_val)"
      ],
      "metadata": {
        "id": "1BkGKOeCXUsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the classification report of model_3 on training and validation data respectively."
      ],
      "metadata": {
        "id": "PMljwQUdcbNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Train data Model_3\", end=\"\\n\\n\")\n",
        "cr_train_model_3 = classification_report(y_train,y_train_pred_3 > 0.5)\n",
        "print(cr_train_model_3)"
      ],
      "metadata": {
        "id": "yUArAjvkXeh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Validation data Model_3\", end=\"\\n\\n\")\n",
        "cr_val_model_3 = classification_report(y_val,y_val_pred_3 > 0.5)\n",
        "print(cr_val_model_3)"
      ],
      "metadata": {
        "id": "mp5gmnufXhmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4"
      ],
      "metadata": {
        "id": "dPAuEVDzX0qL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have used only SGD optimizer till now, let's use another kind of optimizer and observe its impact on the model performmance."
      ],
      "metadata": {
        "id": "ydPkh3AdgWRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "Q9Jx5vOFX0qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the neural network\n",
        "model_4 = Sequential()\n",
        "model_4.add(Dense(_____,activation=\"____\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_4.add(Dense(_____,activation=\"____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_4.add(Dense(_____,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ],
      "metadata": {
        "id": "q_LP0NHzX0qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.summary()"
      ],
      "metadata": {
        "id": "MY_drKH7X0qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()    # defining Adam as the optimizer to be used\n",
        "# model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ],
      "metadata": {
        "id": "vbXDHdCaX0qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "history = model_4.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ],
      "metadata": {
        "id": "Viy8MTihX0qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ],
      "metadata": {
        "id": "uC_tPwLlX0qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(history,'loss')"
      ],
      "metadata": {
        "id": "xJ1TAwlLX0qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the model performance ofr model_4 on training and validation data respectively"
      ],
      "metadata": {
        "id": "E_KuUcLbdKRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_4_train_perf = model_performance_classification(model_4,X_train,y_train)\n",
        "model_4_train_perf"
      ],
      "metadata": {
        "id": "8OvkmGu6X0qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4_val_perf = model_performance_classification(model_4,X_val,y_val)\n",
        "model_4_val_perf"
      ],
      "metadata": {
        "id": "sLE1b8CMX0qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_4 = model_4.predict(X_train)\n",
        "y_val_pred_4 = model_4.predict(X_val)"
      ],
      "metadata": {
        "id": "Wg1dMCXSX0qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the classification report of model_4 on raining and validation data respectively."
      ],
      "metadata": {
        "id": "4OH8cvF2dVjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Train data Model_4\", end=\"\\n\\n\")\n",
        "cr_train_model_4 = classification_report(y_train,y_train_pred_4 > 0.5)\n",
        "print(cr_train_model_4)"
      ],
      "metadata": {
        "id": "D-wxc08wX0qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Validation data Model_4\", end=\"\\n\\n\")\n",
        "cr_val_model_4 = classification_report(y_val,y_val_pred_4 > 0.5)\n",
        "print(cr_val_model_4)"
      ],
      "metadata": {
        "id": "uyKOGZzEX0qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 5"
      ],
      "metadata": {
        "id": "W2dZjgH0Yxok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we will add more layers and dropout while using a different optimizer."
      ],
      "metadata": {
        "id": "x9H_G0rzdwC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "WBKvvEkjYxol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the neural network\n",
        "from tensorflow.keras.layers import Dropout\n",
        "model_5 = Sequential()\n",
        "model_5.add(Dense(_____,activation=\"_____\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_5.add(Dropout(_____)) #Complete the code to define the dropout rate\n",
        "model_5.add(Dense(_____,activation=\"____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_5.add(Dense(_____, activation = \"____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_5.add(Dense(____,activation=\"____\")) # Complete the code to define the number of neurons and activation function"
      ],
      "metadata": {
        "id": "qrPwAVJOYxom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_5.summary()"
      ],
      "metadata": {
        "id": "jU1Q8Z8CYxom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()    # defining Adam as the optimizer to be used\n",
        "# model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ],
      "metadata": {
        "id": "6MQXRa63Yxom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "history = model_5.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ],
      "metadata": {
        "id": "4nfW1HAYYxon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ],
      "metadata": {
        "id": "iT96bkirYxon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(history,'loss')"
      ],
      "metadata": {
        "id": "lV6Ve4KuYxon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the model performance of model_5 on the training and validation data."
      ],
      "metadata": {
        "id": "DL-94xameUkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_5_train_perf = model_performance_classification(model_5,X_train,y_train)\n",
        "model_5_train_perf"
      ],
      "metadata": {
        "id": "LRiDnwPhYxoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_5_val_perf = model_performance_classification(model_5,X_val,y_val)\n",
        "model_5_val_perf"
      ],
      "metadata": {
        "id": "hyz9ix7KYxoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_5 = model_5.predict(X_train)\n",
        "y_val_pred_5 = model_5.predict(X_val)"
      ],
      "metadata": {
        "id": "B0aSAUTlYxop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the classification report of model_5 on training and validation data."
      ],
      "metadata": {
        "id": "68DKFcJIehp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Train data Model_2\", end=\"\\n\\n\")\n",
        "cr_train_model_5 = classification_report(y_train,y_train_pred_5 > 0.5)\n",
        "print(cr_train_model_5)"
      ],
      "metadata": {
        "id": "i9XpJpekYxop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Validation data Model_2\", end=\"\\n\\n\")\n",
        "cr_val_model_5 = classification_report(y_val,y_val_pred_5 > 0.5)\n",
        "print(cr_val_model_5)"
      ],
      "metadata": {
        "id": "9RlRQJOuYxop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 6"
      ],
      "metadata": {
        "id": "H-wDd0RIZQyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how does the model performance change when the model gives higher importance to the minority class"
      ],
      "metadata": {
        "id": "p0IOkrVWYxop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "0LOpw8cNZQyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_6 = Sequential()\n",
        "model_6.add(Dense(____,activation=\"____\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dropout(____)) # Complete the code to define the dropout rate\n",
        "model_6.add(Dense(_____,activation=\"_____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dense(_____, activation = \"_____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dense(_____,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ],
      "metadata": {
        "id": "HNUew9nFZQyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_6.summary()"
      ],
      "metadata": {
        "id": "h_sGGGREZQyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.SGD()\n",
        "# model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ],
      "metadata": {
        "id": "vaVP4CM9ZQyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "history = model_3.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs,class_weight=_____, ) # Complete the code such that the model is biased towards the minority class\n",
        "end=time.time()"
      ],
      "metadata": {
        "id": "fhiZR8WMZQyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ],
      "metadata": {
        "id": "13AF9v2UZQyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(history,'loss')"
      ],
      "metadata": {
        "id": "48uod9TsZQyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the model performance of model_6 on training and validation data."
      ],
      "metadata": {
        "id": "YhlzTJsngBev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_6_train_perf = model_performance_classification(model_6,X_train,y_train)\n",
        "model_6_train_perf"
      ],
      "metadata": {
        "id": "Fiy0gmSUZQyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_6_val_perf = model_performance_classification(model_6,X_val,y_val)\n",
        "model_6_val_perf"
      ],
      "metadata": {
        "id": "aX8Zu4FLZQyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_6 = model_6.predict(X_train)\n",
        "y_val_pred_6 = model_6.predict(X_val)"
      ],
      "metadata": {
        "id": "IXZlBIDLZQyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the classification report of model_6 on both training and validation data."
      ],
      "metadata": {
        "id": "jNsmMIsxgJE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Train data Model_3\", end=\"\\n\\n\")\n",
        "cr_train_model_6 = classification_report(y_train,y_train_pred_6 > 0.5)\n",
        "print(cr_train_model_6)"
      ],
      "metadata": {
        "id": "DXeX1QHSZQyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report - Validation data Model_3\", end=\"\\n\\n\")\n",
        "cr_val_model_6 = classification_report(y_val,y_val_pred_6 > 0.5)\n",
        "print(cr_val_model_6)"
      ],
      "metadata": {
        "id": "SV3-H6ckZQyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Performance Comparison and Final Model Selection**"
      ],
      "metadata": {
        "id": "3O07XgkzY_ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in order to select the final model, we will compare the performances of all the models for the training and test sets."
      ],
      "metadata": {
        "id": "mVYagrg9ZQyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Performance Comparison**"
      ],
      "metadata": {
        "id": "QfZUZD83lu4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training performance comparison\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        model_0_train_perf.T,\n",
        "        model_1_train_perf.T,\n",
        "        model_2_train_perf.T,\n",
        "        model_3_train_perf.T,\n",
        "        model_4_train_perf.T,\n",
        "        model_5_train_perf.T,\n",
        "        model_6_train_perf.T\n",
        "\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_train_comp_df.columns = [\n",
        "    \"Model 0\",\n",
        "    \"Model 1\",\n",
        "    \"Model 2\",\n",
        "    \"Model 3\",\n",
        "    \"Model 4\",\n",
        "    \"Model 5\",\n",
        "    \"Model 6\"\n",
        "]\n",
        "print(\"Training set performance comparison:\")\n",
        "models_train_comp_df"
      ],
      "metadata": {
        "id": "HQB0PVtbh-MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation Performance Comparison**"
      ],
      "metadata": {
        "id": "Jqpgdol7l2Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation performance comparison\n",
        "\n",
        "models_val_comp_df = pd.concat(\n",
        "    [\n",
        "        model_0_val_perf.T,\n",
        "        model_1_val_perf.T,\n",
        "        model_2_val_perf.T,\n",
        "        model_3_val_perf.T,\n",
        "        model_4_val_perf.T,\n",
        "        model_5_val_perf.T,\n",
        "        model_6_val_perf.T\n",
        "\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_val_comp_df.columns = [\n",
        "    \"Model 0\",\n",
        "    \"Model 1\",\n",
        "    \"Model 2\",\n",
        "    \"Model 3\",\n",
        "    \"Model 4\",\n",
        "    \"Model 5\",\n",
        "    \"Model 6\"\n",
        "]\n",
        "print(\"Validation set performance comparison:\")\n",
        "models_val_comp_df"
      ],
      "metadata": {
        "id": "KzWl6tM8ilz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking the performance of the best model on the test set**"
      ],
      "metadata": {
        "id": "q69xcftQuKyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model = model_0 ## Uncomment this line in case the best model is model_0\n",
        "# best_model = model_1 ## Uncomment this line in case the best model is model_1\n",
        "# best_model = model_2 ## Uncomment this line in case the best model is model_2\n",
        "# best_model = model_3 ## Uncomment this line in case the best model is model_3\n",
        "# best_model = model_4 ## Uncomment this line in case the best model is model_4\n",
        "# best_model = model_5 ## Uncomment this line in case the best model is model_5\n",
        "# best_model = model_6 ## Uncomment this line in case the best model is model_6"
      ],
      "metadata": {
        "id": "_pfViqPWq_pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set performance for the best model\n",
        "best_model_test_perf = model_performance_classification(best_model,X_test,y_test)\n",
        "best_model_test_perf"
      ],
      "metadata": {
        "id": "oPxuwyh6j227"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_best = best_model.predict(X_test)\n",
        "\n",
        "cr_test_best_model = classification_report(y_test, y_test_pred_best>0.5) # Check the classification report of best model on test data.\n",
        "print(cr_test_best_model)"
      ],
      "metadata": {
        "id": "J4ZzDS8Cqe_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdME3J172AN-"
      },
      "source": [
        "# **Actionable Insights and Recommendations**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Write down actionable insights here"
      ],
      "metadata": {
        "id": "IkJU-1M5uT5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Write down business recommendations here"
      ],
      "metadata": {
        "id": "-O4lLWtluXz7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "x3-QehJxbp0t",
        "4p9zp6bHfPOY"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}